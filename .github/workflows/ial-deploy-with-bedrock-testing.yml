name: IaL Deploy with Bedrock Intelligent Testing

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

env:
  PROJECT_NAME: ial
  AWS_REGION: us-east-1
  ENVIRONMENT: ${{ github.ref == 'refs/heads/main' && 'production' || 'staging' }}

jobs:
  deploy-and-test:
    runs-on: ubuntu-latest
    
    permissions:
      id-token: write
      contents: read
      pull-requests: write
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
      
      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
      
      - name: Install dependencies
        run: |
          pip install boto3 pytest pytest-json-report
          sudo apt-get update && sudo apt-get install -y awscli
      
      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: arn:aws:iam::${{ secrets.AWS_ACCOUNT_ID }}:role/IaL-GitHubActionsRole
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Verify AWS connection
        run: |
          aws sts get-caller-identity
          echo "AWS_ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)" >> $GITHUB_ENV
      
      - name: Deploy Infrastructure (Phases 00-17)
        run: |
          echo "üöÄ Deploying IaL infrastructure..."
          
          # Deploy each phase sequentially
          for phase in phases/*.yaml; do
            phase_name=$(basename "$phase" .yaml)
            echo "üì¶ Deploying $phase_name..."
            
            # Here you would call your deployment logic
            # For now, we'll simulate with a status update
            python3 -c "
          import boto3
          import json
          from datetime import datetime
          
          dynamodb = boto3.client('dynamodb')
          
          # Update deployment status
          dynamodb.put_item(
              TableName='mcp-provisioning-checklist',
              Item={
                  'Project': {'S': '${{ env.PROJECT_NAME }}'},
                  'ResourceName': {'S': '$phase_name'},
                  'Status': {'S': 'Created'},
                  'Phase': {'S': '$phase_name'},
                  'Timestamp': {'S': datetime.utcnow().isoformat()},
                  'Executor': {'S': 'github-actions'},
                  'ResourceType': {'S': 'AWS::CloudFormation::Stack'}
              }
          )
          print(f'‚úÖ {phase_name} deployment recorded')
          "
          done
      
      - name: Generate Bedrock Tests
        run: |
          echo "üß† Generating intelligent tests with Bedrock..."
          mkdir -p logs reports tests/generated
          
          export PROJECT_NAME=${{ env.PROJECT_NAME }}
          export AWS_ACCOUNT_ID=${{ env.AWS_ACCOUNT_ID }}
          
          python3 scripts/bedrock-test-generator.py
      
      - name: Execute Bedrock Tests
        id: test_execution
        run: |
          echo "üß™ Executing Bedrock-generated tests..."
          
          export PROJECT_NAME=${{ env.PROJECT_NAME }}
          export AWS_ACCOUNT_ID=${{ env.AWS_ACCOUNT_ID }}
          export ENVIRONMENT=${{ env.ENVIRONMENT }}
          
          # Execute tests and capture exit code
          python3 scripts/bedrock-test-executor.py
          echo "test_exit_code=$?" >> $GITHUB_OUTPUT
      
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: bedrock-test-results
          path: |
            tests/generated/
            reports/
            logs/bedrock_usage.jsonl
      
      - name: Comment PR with Test Results
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v6
        with:
          script: |
            const fs = require('fs');
            
            try {
              const analysis = JSON.parse(fs.readFileSync('reports/test_analysis.json', 'utf8'));
              
              const comment = `## üß† Bedrock Intelligent Test Results
              
              **Overall Health:** ${analysis.analysis.overall_health}
              **Health Score:** ${analysis.analysis.health_score}/100
              **Recommendation:** ${analysis.deployment_recommendation}
              
              ### Critical Issues (${analysis.analysis.critical_issues.length})
              ${analysis.analysis.critical_issues.map(issue => 
                `- **${issue.severity.toUpperCase()}:** ${issue.description}`
              ).join('\n') || 'None'}
              
              ### Auto-Fixed Issues (${analysis.analysis.auto_fixable_issues.length})
              ${analysis.analysis.auto_fixable_issues.map(issue => 
                `- ‚úÖ ${issue.issue}`
              ).join('\n') || 'None'}
              
              ### Recommendations
              ${analysis.analysis.recommendations.map(rec => `- ${rec}`).join('\n')}
              
              ---
              *Generated by Bedrock AI at ${analysis.timestamp}*`;
              
              github.rest.issues.createComment({
                issue_number: context.issue.number,
                owner: context.repo.owner,
                repo: context.repo.repo,
                body: comment
              });
            } catch (error) {
              console.log('Could not read test analysis file:', error.message);
            }
      
      - name: Check Deployment Health
        run: |
          if [ -f "reports/test_analysis.json" ]; then
            health=$(python3 -c "
          import json
          with open('reports/test_analysis.json', 'r') as f:
              data = json.load(f)
          print(data['analysis']['overall_health'])
          ")
            
            echo "Deployment health: $health"
            
            if [ "$health" = "CRITICAL" ]; then
              echo "‚ùå Deployment failed health check"
              exit 1
            elif [ "$health" = "DEGRADED" ]; then
              echo "‚ö†Ô∏è Deployment has issues but proceeding"
              exit 0
            else
              echo "‚úÖ Deployment is healthy"
              exit 0
            fi
          else
            echo "‚ö†Ô∏è No test analysis available"
            exit 0
          fi
      
      - name: Generate Cost Report
        if: always()
        run: |
          echo "üí∞ Generating Bedrock usage cost report..."
          
          python3 -c "
          import json
          import os
          
          total_cost = 0
          usage_count = 0
          
          if os.path.exists('logs/bedrock_usage.jsonl'):
              with open('logs/bedrock_usage.jsonl', 'r') as f:
                  for line in f:
                      if line.strip():
                          usage = json.loads(line)
                          
                          # Calculate cost based on model and tokens
                          if 'sonnet' in usage['model_id']:
                              input_cost = usage['input_tokens'] * 3.00 / 1000000
                              output_cost = usage['output_tokens'] * 15.00 / 1000000
                          else:  # haiku
                              input_cost = usage['input_tokens'] * 0.25 / 1000000
                              output_cost = usage['output_tokens'] * 1.25 / 1000000
                          
                          total_cost += input_cost + output_cost
                          usage_count += 1
          
          print(f'üìä Bedrock Usage Summary:')
          print(f'   API Calls: {usage_count}')
          print(f'   Total Cost: \${total_cost:.4f}')
          print(f'   Cost per Deploy: \${total_cost:.4f}')
          "
      
      - name: Notify Deployment Status
        if: always()
        run: |
          status="${{ job.status }}"
          
          if [ "$status" = "success" ]; then
            echo "‚úÖ IaL deployment with Bedrock testing completed successfully"
          else
            echo "‚ùå IaL deployment with Bedrock testing failed"
          fi
