"""
LLM-Powered Clarification Engine
Usa LLM + MCP para detectar requisitos faltantes e fazer perguntas inteligentes
"""

import json
from typing import Dict, List, Any, Optional

class LLMClarificationEngine:
    def __init__(self, llm_provider, mcp_orchestrator):
        self.llm_provider = llm_provider
        self.mcp_orchestrator = mcp_orchestrator
        
        self.clarification_prompt = """
Voc√™ √© um especialista AWS que ajuda usu√°rios a especificar requisitos de infraestrutura.

TAREFA: Analisar a solicita√ß√£o do usu√°rio e identificar informa√ß√µes faltantes cr√≠ticas.

REGRAS:
1. Se a solicita√ß√£o est√° COMPLETA e espec√≠fica ‚Üí retorne {"needs_clarification": false}
2. Se faltam informa√ß√µes CR√çTICAS ‚Üí retorne {"needs_clarification": true, "questions": [...]}
3. Fa√ßa NO M√ÅXIMO 3 perguntas mais importantes
4. Perguntas devem ser espec√≠ficas e t√©cnicas
5. Inclua op√ß√µes quando apropriado

EXEMPLOS:
- "crie um bucket s3" ‚Üí COMPLETO (bucket b√°sico √© suficiente)
- "crie uma ecs" ‚Üí INCOMPLETO (falta task definition, networking, etc.)
- "preciso de database" ‚Üí INCOMPLETO (tipo, tamanho, uso, etc.)

FORMATO DE RESPOSTA:
{{
  "needs_clarification": true,
  "confidence": 0.5,
  "questions": [
    {{
      "question": "Pergunta espec√≠fica?",
      "context": "Por que essa informa√ß√£o √© importante",
      "options": ["Op√ß√£o 1", "Op√ß√£o 2", "Op√ß√£o 3"]
    }}
  ],
  "reasoning": "Por que essas perguntas s√£o necess√°rias"
}}

SOLICITA√á√ÉO DO USU√ÅRIO: {user_request}
"""

    async def analyze_and_clarify(self, user_request: str) -> Dict[str, Any]:
        """Usa LLM para analisar requisitos e gerar perguntas inteligentes"""
        
        print(f"üîç DEBUG LLM: Analisando requisito: {user_request}")
        
        # Verificar se LLM provider est√° dispon√≠vel
        if not self.llm_provider:
            print(f"‚ö†Ô∏è LLM provider n√£o dispon√≠vel, usando fallback MCP")
            return await self._fallback_mcp_analysis(user_request)
        
        # Usar LLM para an√°lise inteligente
        print(f"üîç DEBUG LLM: Preparando prompt...")
        try:
            prompt = self.clarification_prompt.format(user_request=user_request)
            print(f"üîç DEBUG LLM: Prompt preparado, tamanho: {len(prompt)} chars")
        except Exception as prompt_error:
            print(f"‚ö†Ô∏è Erro ao formatar prompt: {prompt_error}, usando fallback MCP")
            return await self._fallback_mcp_analysis(user_request)
        
        print(f"üîç DEBUG LLM: Entrando no try block...")
        try:
            print(f"üîç DEBUG LLM: Enviando para LLM...")
            
            # Implementar timeout robusto para evitar travamento
            import asyncio
            try:
                llm_response = await asyncio.wait_for(
                    self.llm_provider.process_natural_language_async(prompt),
                    timeout=15.0  # 15 segundos timeout
                )
                print(f"üîç DEBUG LLM: Resposta recebida: {type(llm_response)}")
            except asyncio.TimeoutError:
                print(f"‚ö†Ô∏è LLM timeout ap√≥s 15s, usando fallback MCP")
                return await self._fallback_mcp_analysis(user_request)
            except Exception as llm_error:
                print(f"‚ö†Ô∏è Erro espec√≠fico do LLM: {llm_error}, usando fallback MCP")
                return await self._fallback_mcp_analysis(user_request)
            
            # Parse da resposta LLM com error handling robusto
            try:
                if isinstance(llm_response, str):
                    # Tentar extrair JSON da resposta se estiver em markdown ou texto
                    import re
                    json_match = re.search(r'\{.*\}', llm_response, re.DOTALL)
                    if json_match:
                        analysis = json.loads(json_match.group())
                    else:
                        # Se n√£o encontrar JSON, usar fallback MCP
                        print(f"üîç DEBUG LLM: Resposta n√£o cont√©m JSON v√°lido, usando fallback MCP")
                        return await self._fallback_mcp_analysis(user_request)
                else:
                    analysis = llm_response
                    
                print(f"üîç DEBUG LLM: An√°lise parseada: {analysis.get('needs_clarification')}")
                
            except (json.JSONDecodeError, KeyError, AttributeError) as parse_error:
                print(f"‚ö†Ô∏è Erro no parsing da resposta LLM: {parse_error}, usando fallback MCP")
                return await self._fallback_mcp_analysis(user_request)
            
            # Se n√£o precisa clarifica√ß√£o, retorna direto
            if not analysis.get('needs_clarification', False):
                print(f"üîç DEBUG LLM: Requisitos suficientes, prosseguindo")
                return {
                    'status': 'ready_to_generate',
                    'confidence': analysis.get('confidence', 0.8),
                    'reasoning': analysis.get('reasoning', 'Requisitos suficientes')
                }
            
            # Se precisa clarifica√ß√£o, formatar perguntas
            questions = analysis.get('questions', [])
            if not questions:
                print(f"üîç DEBUG LLM: LLM n√£o gerou perguntas, usando fallback MCP")
                # Fallback se LLM n√£o gerou perguntas
                return await self._fallback_mcp_analysis(user_request)
            
            print(f"üîç DEBUG LLM: Formatando {len(questions)} perguntas")
            formatted_response = self._format_clarification_questions(questions, user_request)
            
            return {
                'status': 'needs_clarification',
                'response': formatted_response,
                'questions': questions,
                'confidence': analysis.get('confidence', 0.3),
                'reasoning': analysis.get('reasoning', 'Informa√ß√µes adicionais necess√°rias')
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro geral na an√°lise LLM: {e}")
            print(f"üîç DEBUG LLM: Usando fallback MCP por erro geral")
            # Fallback para an√°lise MCP - NUNCA FALHA
            return await self._fallback_mcp_analysis(user_request)
    
    async def _fallback_mcp_analysis(self, user_request: str) -> Dict[str, Any]:
        """Fallback usando MCP para an√°lise quando LLM falha - ENTERPRISE GRADE"""
        try:
            print(f"üîç DEBUG MCP: Iniciando an√°lise MCP para: {user_request}")
            
            # Usar MCP para detectar servi√ßos e gaps
            mcp_analysis = await self.mcp_orchestrator.analyze_requirements(user_request)
            print(f"üîç DEBUG MCP: An√°lise MCP completa: {mcp_analysis.get('complete')}")
            print(f"üîç DEBUG MCP: Servi√ßos detectados: {mcp_analysis.get('detected_services')}")
            print(f"üîç DEBUG MCP: Informa√ß√µes faltantes: {mcp_analysis.get('missing_info')}")
            
            if mcp_analysis.get('complete', False):
                print(f"üîç DEBUG MCP: Requisitos completos, prosseguindo com gera√ß√£o")
                return {
                    'status': 'ready_to_generate',
                    'confidence': mcp_analysis.get('confidence', 0.7),
                    'reasoning': 'MCP analysis indicates complete requirements'
                }
            
            # Gerar perguntas baseadas em gaps do MCP
            gaps = mcp_analysis.get('missing_info', [])
            primary_service = mcp_analysis.get('primary_service', 'unknown')
            
            if not gaps:
                print(f"üîç DEBUG MCP: Nenhum gap detectado, prosseguindo com gera√ß√£o")
                return {
                    'status': 'ready_to_generate',
                    'confidence': 0.6,
                    'reasoning': 'No specific gaps detected by MCP'
                }
            
            print(f"üîç DEBUG MCP: Gerando perguntas para {len(gaps)} gaps")
            questions = self._generate_questions_from_gaps(gaps, primary_service, user_request)
            
            if not questions:
                print(f"üîç DEBUG MCP: Nenhuma pergunta gerada, prosseguindo com gera√ß√£o")
                return {
                    'status': 'ready_to_generate',
                    'confidence': 0.5,
                    'reasoning': 'MCP could not generate specific questions'
                }
            
            print(f"üîç DEBUG MCP: Formatando {len(questions)} perguntas")
            formatted_response = self._format_clarification_questions(questions, user_request)
            
            return {
                'status': 'needs_clarification',
                'response': formatted_response,
                'questions': questions,
                'confidence': mcp_analysis.get('confidence', 0.5),
                'reasoning': f'MCP detected missing information for {primary_service}: {gaps}',
                'mcp_analysis': mcp_analysis
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro no fallback MCP: {e}")
            print(f"üîç DEBUG MCP: Erro cr√≠tico, usando √∫ltimo fallback")
            # √öltimo fallback - NUNCA FALHA, sempre permite prosseguir
            return {
                'status': 'ready_to_generate',
                'confidence': 0.4,
                'reasoning': f'Emergency fallback - proceeding with available information. Error: {str(e)}'
            }
    
    def _generate_questions_from_gaps(self, gaps: List[str], primary_service: str, user_request: str) -> List[Dict[str, Any]]:
        """Gera perguntas baseadas nos gaps identificados pelo MCP"""
        questions = []
        
        # Mapeamento de gaps para perguntas por servi√ßo
        service_gap_questions = {
            'ecs': {
                'task_definition': {
                    'question': 'Qual aplica√ß√£o voc√™ quer containerizar?',
                    'context': 'Preciso saber a imagem Docker, CPU, mem√≥ria e portas',
                    'options': ['Aplica√ß√£o web (nginx/apache)', 'API backend (node/python)', 'Worker/batch job', 'Microservi√ßo customizado']
                },
                'networking': {
                    'question': 'Como ser√° o acesso de rede?',
                    'context': 'Define se usa VPC p√∫blica, privada ou load balancer',
                    'options': ['P√∫blico com ALB', 'Privado (VPC)', 'Sem acesso externo']
                },
                'scaling': {
                    'question': 'Quantas inst√¢ncias voc√™ precisa?',
                    'context': 'Define configura√ß√µes de auto scaling',
                    'options': ['1 inst√¢ncia (desenvolvimento)', '2-5 inst√¢ncias (produ√ß√£o)', 'Auto scaling baseado em CPU']
                }
            },
            'rds': {
                'database_engine': {
                    'question': 'Qual engine de banco voc√™ prefere?',
                    'context': 'Cada engine tem caracter√≠sticas diferentes',
                    'options': ['MySQL (compatibilidade)', 'PostgreSQL (recursos avan√ßados)', 'Aurora (performance)']
                },
                'instance_size': {
                    'question': 'Qual o tamanho esperado do banco?',
                    'context': 'Define tipo de inst√¢ncia e storage',
                    'options': ['Pequeno (db.t3.micro)', 'M√©dio (db.t3.small)', 'Grande (db.m5.large)']
                },
                'availability': {
                    'question': 'Precisa de alta disponibilidade?',
                    'context': 'Multi-AZ aumenta disponibilidade mas dobra o custo',
                    'options': ['Sim, cr√≠tico (Multi-AZ)', 'N√£o, desenvolvimento (Single-AZ)', 'Backup apenas']
                }
            },
            'lambda': {
                'runtime': {
                    'question': 'Qual linguagem voc√™ vai usar?',
                    'context': 'Define o runtime environment',
                    'options': ['Python 3.11', 'Node.js 18', 'Java 17', 'Go 1.x']
                },
                'performance_config': {
                    'question': 'Qual performance voc√™ precisa?',
                    'context': 'Define mem√≥ria, timeout e concorr√™ncia',
                    'options': ['Baixa (128MB, 3s)', 'M√©dia (512MB, 15s)', 'Alta (3GB, 15min)']
                }
            }
        }
        
        # Perguntas gen√©ricas para gaps n√£o mapeados
        generic_questions = {
            'instance_type': {
                'question': 'Qual tipo de inst√¢ncia voc√™ precisa?',
                'context': 'Isso afeta performance e custo',
                'options': ['t3.micro (desenvolvimento)', 't3.small (teste)', 'm5.large (produ√ß√£o)']
            },
            'networking': {
                'question': 'Como ser√° o acesso de rede?',
                'context': 'Define configura√ß√µes de VPC e security groups',
                'options': ['P√∫blico (internet)', 'Privado (VPC)', 'H√≠brido']
            },
            'storage': {
                'question': 'Que tipo de storage voc√™ precisa?',
                'context': 'Diferentes tipos t√™m diferentes performance e custos',
                'options': ['GP3 (geral)', 'IO2 (alta performance)', 'ST1 (throughput)']
            }
        }
        
        # Usar perguntas espec√≠ficas do servi√ßo se dispon√≠vel
        service_questions = service_gap_questions.get(primary_service, {})
        
        for gap in gaps[:3]:  # M√°ximo 3 perguntas
            if gap in service_questions:
                questions.append(service_questions[gap])
            elif gap in generic_questions:
                questions.append(generic_questions[gap])
            else:
                # Pergunta gen√©rica para gaps n√£o mapeados
                questions.append({
                    'question': f'Voc√™ pode especificar mais detalhes sobre {gap.replace("_", " ")}?',
                    'context': 'Essa informa√ß√£o √© necess√°ria para gerar a configura√ß√£o correta',
                    'options': []
                })
        
        return questions
    
    def _format_clarification_questions(self, questions: List[Dict[str, Any]], user_request: str) -> str:
        """Formata perguntas para exibi√ß√£o ao usu√°rio"""
        response = f"ü§î **Preciso de mais detalhes sobre: '{user_request}'**\n\n"
        
        for i, q in enumerate(questions, 1):
            response += f"**{i}. {q['question']}**\n"
            
            if 'options' in q and q['options']:
                for j, option in enumerate(q['options'], 1):
                    response += f"   {j}) {option}\n"
            
            if 'context' in q and q['context']:
                response += f"   üí° *{q['context']}*\n"
            
            response += "\n"
        
        response += "üìù **Responda com detalhes ou n√∫meros das op√ß√µes para prosseguir.**"
        
        return response
    
    async def process_clarification_response(self, user_response: str, original_request: str) -> Dict[str, Any]:
        """Processa resposta do usu√°rio e combina com requisito original"""
        
        # Usar LLM para combinar requisito original + clarifica√ß√µes
        combine_prompt = f"""
Combine o requisito original com as clarifica√ß√µes do usu√°rio em um requisito completo e espec√≠fico.

REQUISITO ORIGINAL: {original_request}
CLARIFICA√á√ïES: {user_response}

TAREFA: Criar um requisito √∫nico, completo e espec√≠fico que pode ser usado para gerar templates AWS.

FORMATO: Retorne apenas o requisito combinado, claro e t√©cnico.
"""
        
        try:
            combined_requirement = await self.llm_provider.process_natural_language_async(combine_prompt)
            
            return {
                'status': 'clarified',
                'combined_requirement': combined_requirement,
                'ready_to_generate': True
            }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Erro ao combinar requisitos: {e}")
            # Fallback simples
            return {
                'status': 'clarified',
                'combined_requirement': f"{original_request}. Detalhes adicionais: {user_response}",
                'ready_to_generate': True
            }
