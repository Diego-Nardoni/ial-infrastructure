#!/usr/bin/env python3
"""
LLM Clarification Engine - REAL LLM + MCP Implementation
"""

import asyncio
from typing import Dict, List, Any, Optional
import json
import re

class LLMClarificationEngine:
    """Engine para an√°lise inteligente de requisitos usando LLM + MCP REAL"""
    
    def __init__(self, llm_provider, mcp_orchestrator):
        self.llm_provider = llm_provider
        self.mcp_orchestrator = mcp_orchestrator
    
    async def analyze_and_clarify(self, user_request: str) -> Dict[str, Any]:
        """Usa LLM REAL para analisar requisitos e gerar perguntas inteligentes"""
        
        # BYPASS: Comandos espec√≠ficos que n√£o precisam clarifica√ß√£o
        if self._has_sufficient_details(user_request):
            return {
                'status': 'ready_to_generate',
                'confidence': 0.9,
                'reasoning': 'Command has sufficient details for generation'
            }
        
        # PRIMEIRO: Verificar se √© uma resposta a pergunta anterior
        if self._is_answer_to_question(user_request):
            return {
                'status': 'ready_to_generate',
                'confidence': 0.8,
                'reasoning': 'User provided answer to clarification question'
            }
        
        # USAR LLM REAL para an√°lise inteligente
        try:
            print("üß† Using REAL LLM for analysis...")
            analysis = await self._analyze_with_real_llm(user_request)
            
            if analysis.get('status') == 'needs_clarification':
                # Gerar perguntas inteligentes via LLM+MCP REAL
                questions = await self._generate_real_intelligent_questions(user_request, analysis)
                
                if questions:
                    formatted_response = self._format_clarification_questions(questions, user_request)
                    return {
                        'status': 'needs_clarification',
                        'response': formatted_response,
                        'questions': questions,
                        'confidence': analysis.get('confidence', 0.7),
                        'reasoning': 'REAL LLM+MCP analysis completed',
                        'llm_used': True
                    }
            
            return analysis
            
        except Exception as e:
            print(f"‚ö†Ô∏è REAL LLM failed: {e}, using intelligent fallback")
            return await self._intelligent_fallback_analysis(user_request)
    
    async def _analyze_with_real_llm(self, user_request: str) -> Dict[str, Any]:
        """An√°lise REAL usando Bedrock com prompt engineering otimizado"""
        
        # PROMPT ENGINEERING ESPEC√çFICO PARA BEDROCK
        bedrock_prompt = f"""
Voc√™ √© um especialista AWS que analisa requisitos de infraestrutura.

TAREFA: Analise se este requisito precisa de clarifica√ß√£o adicional.

REQUISITO: "{user_request}"

CRIT√âRIOS:
- Se faltam detalhes t√©cnicos espec√≠ficos ‚Üí needs_clarification
- Se tem informa√ß√µes suficientes para implementar ‚Üí ready_to_generate

RESPONDA EXATAMENTE neste formato JSON:
{{
    "status": "needs_clarification",
    "confidence": 0.8,
    "reasoning": "Faltam detalhes sobre tipo de banco e volume",
    "missing_details": ["database_type", "performance_requirements", "scaling_needs"]
}}

OU

{{
    "status": "ready_to_generate",
    "confidence": 0.9,
    "reasoning": "Requisito tem detalhes suficientes"
}}

AN√ÅLISE:
"""
        
        try:
            # Usar LLM Provider REAL
            llm_response = await self.llm_provider.generate_response(bedrock_prompt)
            
            # PARSING ROBUSTO da resposta
            analysis = self._parse_llm_response(llm_response)
            
            print(f"‚úÖ REAL LLM Analysis: {analysis.get('status')} (confidence: {analysis.get('confidence')})")
            return analysis
            
        except Exception as e:
            print(f"‚ö†Ô∏è LLM analysis error: {e}")
            raise e
    
    def _parse_llm_response(self, llm_response: str) -> Dict[str, Any]:
        """Parse robusto da resposta do LLM com m√∫ltiplos fallbacks"""
        
        try:
            # Tentar parsing direto se for string JSON
            if isinstance(llm_response, str) and llm_response.strip().startswith('{'):
                return json.loads(llm_response)
            
            # Se for dict do LLM Provider, extrair texto
            if isinstance(llm_response, dict):
                text = llm_response.get('response', llm_response.get('processed_text', str(llm_response)))
            else:
                text = str(llm_response)
            
            # Extrair JSON da resposta usando regex
            json_match = re.search(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text)
            if json_match:
                json_str = json_match.group()
                return json.loads(json_str)
            
            # Fallback: an√°lise baseada em keywords
            return self._keyword_based_analysis(text)
            
        except Exception as e:
            print(f"‚ö†Ô∏è Parse error: {e}, using keyword analysis")
            return self._keyword_based_analysis(str(llm_response))
    
    def _keyword_based_analysis(self, text: str) -> Dict[str, Any]:
        """An√°lise baseada em keywords quando JSON parsing falha"""
        
        text_lower = text.lower()
        
        # Indicadores de que precisa clarifica√ß√£o
        needs_clarification_indicators = [
            'needs_clarification', 'precisa', 'falta', 'missing', 'unclear',
            'specify', 'details', 'more information'
        ]
        
        # Indicadores de que est√° pronto
        ready_indicators = [
            'ready_to_generate', 'sufficient', 'complete', 'clear',
            'enough information', 'ready'
        ]
        
        if any(indicator in text_lower for indicator in needs_clarification_indicators):
            return {
                'status': 'needs_clarification',
                'confidence': 0.6,
                'reasoning': 'LLM indicated clarification needed (keyword analysis)'
            }
        elif any(indicator in text_lower for indicator in ready_indicators):
            return {
                'status': 'ready_to_generate',
                'confidence': 0.7,
                'reasoning': 'LLM indicated ready to generate (keyword analysis)'
            }
        else:
            # Default: assume needs clarification for safety
            return {
                'status': 'needs_clarification',
                'confidence': 0.5,
                'reasoning': 'Unclear LLM response, defaulting to clarification'
            }
    
    async def _generate_real_intelligent_questions(self, user_request: str, analysis: Dict) -> List[Dict[str, Any]]:
        """Gera perguntas REAIS usando LLM + MCP consultation"""
        
        questions = []
        
        # 1. WORKLOAD_NAME sempre hardcoded (necess√°rio para organiza√ß√£o)
        questions.append({
            'question': 'Qual o nome do workload/projeto?',
            'context': 'Usado para organizar arquivos em /phases/workloads/{nome}',
            'options': ['Nome personalizado (ex: api-backend)', 'Gerar automaticamente', 'Usar estrutura atual (99-misc)'],
            'source': 'hardcoded'
        })
        
        # 2. DETECTAR SERVI√áO PRINCIPAL
        primary_service = self._detect_primary_service(user_request)
        print(f"üéØ Primary service detected: {primary_service}")
        
        # 3. CONSULTAR MCP SERVER REAL para contexto espec√≠fico
        mcp_context = await self._get_real_mcp_context(primary_service)
        print(f"üì° MCP context obtained: {len(mcp_context)} chars")
        
        # 4. USAR LLM REAL para gerar perguntas contextuais
        llm_questions = await self._generate_llm_questions_real(user_request, primary_service, mcp_context, analysis)
        questions.extend(llm_questions)
        
        return questions[:3]  # M√°ximo 3 perguntas
    
    async def _get_real_mcp_context(self, service: str) -> str:
        """Consulta MCP Server REAL para obter contexto espec√≠fico do servi√ßo"""
        
        # Mapeamento de servi√ßos para MCPs espec√≠ficos REAIS
        service_mcp_mapping = {
            'rds': 'aws-rds-mcp',
            'dynamodb': 'aws-dynamodb-mcp', 
            'ecs': 'awslabs.ecs-mcp-server',
            's3': 'aws-s3-mcp',
            'lambda': 'aws-lambda-mcp',
            'elasticache': 'awslabs.elasticache-mcp-server',
            'vpc': 'aws-vpc-mcp',
            'ec2': 'aws-ec2-mcp'
        }
        
        mcp_server = service_mcp_mapping.get(service, 'aws-general-mcp')
        
        try:
            print(f"üì° Querying MCP server: {mcp_server} for {service}")
            
            # Consultar MCP REAL
            context = await self.mcp_orchestrator.query_mcp_for_service_options(mcp_server, service)
            
            # Se contexto muito gen√©rico, enriquecer com conhecimento espec√≠fico
            if len(context) < 100:
                context = self._enrich_service_context(service, context)
            
            return context
                
        except Exception as e:
            print(f"‚ö†Ô∏è MCP query error: {e}")
            return self._get_fallback_service_context(service)
    
    def _enrich_service_context(self, service: str, basic_context: str) -> str:
        """Enriquece contexto b√°sico com conhecimento espec√≠fico do servi√ßo"""
        
        enriched_contexts = {
            'rds': f"{basic_context}\n\nRDS Options: MySQL (5.7, 8.0), PostgreSQL (13, 14, 15), Aurora MySQL, Aurora PostgreSQL. Multi-AZ for HA, Read Replicas for scaling. Instance types: db.t3.micro (dev), db.r5.large (prod).",
            
            'dynamodb': f"{basic_context}\n\nDynamoDB: NoSQL serverless, pay-per-request or provisioned capacity. Global Tables for multi-region. On-Demand for unpredictable workloads, Provisioned for consistent traffic.",
            
            'ecs': f"{basic_context}\n\nECS Options: Fargate (serverless containers) or EC2 (managed instances). Fargate: no server management, higher cost. EC2: more control, lower cost. ALB for load balancing.",
            
            's3': f"{basic_context}\n\nS3 Storage Classes: Standard (frequent access), IA (infrequent), Glacier (archive). Static website hosting with CloudFront CDN. Versioning and lifecycle policies available.",
            
            'lambda': f"{basic_context}\n\nLambda: Serverless functions, 15min max runtime. Memory: 128MB-10GB. Runtimes: Python 3.11, Node.js 18, Java 17. Triggers: API Gateway, S3, DynamoDB, EventBridge.",
            
            'elasticache': f"{basic_context}\n\nElastiCache: Redis (data structures, persistence) or Memcached (simple caching). Redis Cluster for scaling. Multi-AZ for HA. Node types: cache.t3.micro (dev), cache.r6g.large (prod)."
        }
        
        return enriched_contexts.get(service, f"{basic_context}\n\nAWS {service} service with multiple configuration options available.")
    
    def _get_fallback_service_context(self, service: str) -> str:
        """Contexto fallback quando MCP falha"""
        
        fallback_contexts = {
            'rds': 'RDS: Managed relational databases (MySQL, PostgreSQL, Aurora). Choose Multi-AZ for high availability, Read Replicas for read scaling.',
            'dynamodb': 'DynamoDB: NoSQL serverless database. On-Demand pricing for variable workloads, Provisioned for predictable traffic.',
            'ecs': 'ECS: Container orchestration. Fargate for serverless containers, EC2 for more control and lower costs.',
            's3': 'S3: Object storage with multiple storage classes. Can host static websites with CloudFront for global distribution.',
            'lambda': 'Lambda: Serverless compute for event-driven applications. Multiple runtime options and trigger sources.',
            'elasticache': 'ElastiCache: In-memory caching with Redis or Memcached. Redis for advanced features, Memcached for simple caching.'
        }
        
        return fallback_contexts.get(service, f'AWS {service} service with configurable options for different use cases.')
    
    async def _generate_llm_questions_real(self, user_request: str, primary_service: str, mcp_context: str, analysis: Dict) -> List[Dict[str, Any]]:
        """Usa LLM REAL para gerar perguntas espec√≠ficas e inteligentes"""
        
        # PROMPT ENGINEERING OTIMIZADO PARA BEDROCK
        llm_prompt = f"""
Voc√™ √© um especialista AWS. Gere 2 perguntas espec√≠ficas para completar este requisito de infraestrutura.

REQUISITO: {user_request}
SERVI√áO PRINCIPAL: {primary_service}
CONTEXTO MCP: {mcp_context}

GERE perguntas que ajudem a escolher entre op√ß√µes REAIS do AWS {primary_service}:
- Perguntas espec√≠ficas com op√ß√µes t√©cnicas reais
- Contexto sobre trade-offs (custo/performance/complexidade)
- Op√ß√µes pr√°ticas baseadas no contexto MCP

FORMATO EXATO (JSON v√°lido):
[
  {{
    "question": "Pergunta espec√≠fica sobre {primary_service}?",
    "context": "Explica√ß√£o t√©cnica do trade-off",
    "options": ["Op√ß√£o 1 real", "Op√ß√£o 2 real", "Op√ß√£o 3 real"]
  }},
  {{
    "question": "Segunda pergunta espec√≠fica?",
    "context": "Contexto t√©cnico",
    "options": ["Op√ß√£o A", "Op√ß√£o B", "Op√ß√£o C"]
  }}
]

PERGUNTAS:
"""

        try:
            print("üß† Generating questions with REAL LLM...")
            llm_response = await self.llm_provider.generate_response(llm_prompt)
            
            # Parse resposta do LLM
            questions = self._parse_llm_questions(llm_response)
            
            if questions:
                print(f"‚úÖ Generated {len(questions)} intelligent questions via LLM")
                for q in questions:
                    q['source'] = 'llm_mcp'
                return questions
            else:
                print("‚ö†Ô∏è LLM questions parsing failed, using intelligent fallback")
                return self._get_intelligent_fallback_questions(primary_service)
                
        except Exception as e:
            print(f"‚ö†Ô∏è LLM questions error: {e}")
            return self._get_intelligent_fallback_questions(primary_service)
    
    def _parse_llm_questions(self, llm_response: str) -> List[Dict[str, Any]]:
        """Parse robusto das perguntas geradas pelo LLM"""
        
        try:
            # Se for dict do LLM Provider, extrair texto
            if isinstance(llm_response, dict):
                text = llm_response.get('response', llm_response.get('processed_text', str(llm_response)))
            else:
                text = str(llm_response)
            
            # Tentar extrair array JSON
            json_match = re.search(r'\[.*\]', text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                questions = json.loads(json_str)
                
                # Validar estrutura das perguntas
                valid_questions = []
                for q in questions:
                    if isinstance(q, dict) and 'question' in q:
                        valid_questions.append({
                            'question': q.get('question', ''),
                            'context': q.get('context', ''),
                            'options': q.get('options', [])
                        })
                
                return valid_questions
            
            return []
            
        except Exception as e:
            print(f"‚ö†Ô∏è Questions parse error: {e}")
            return []
    
    def _get_intelligent_fallback_questions(self, service: str) -> List[Dict[str, Any]]:
        """Perguntas fallback inteligentes por servi√ßo quando LLM falha"""
        
        intelligent_fallbacks = {
            'rds': [
                {
                    'question': 'Voc√™ prefere RDS (gerenciado) ou DynamoDB (NoSQL)?',
                    'context': 'RDS √© melhor para dados relacionais com ACID, DynamoDB para alta escala e performance',
                    'options': ['RDS MySQL/PostgreSQL', 'RDS Aurora (recomendado)', 'DynamoDB NoSQL']
                },
                {
                    'question': 'Precisa de alta disponibilidade (Multi-AZ)?',
                    'context': 'Multi-AZ duplica custos mas garante 99.95% uptime com failover autom√°tico',
                    'options': ['Sim, Multi-AZ (produ√ß√£o)', 'N√£o, Single-AZ (desenvolvimento)', 'Read Replicas apenas']
                }
            ],
            'ecs': [
                {
                    'question': 'Voc√™ prefere Fargate (serverless) ou EC2 (controle total)?',
                    'context': 'Fargate √© mais simples e sem gerenciamento, EC2 oferece mais controle e menor custo',
                    'options': ['Fargate (recomendado)', 'EC2 com Auto Scaling', 'EC2 Spot para economia']
                },
                {
                    'question': 'Como ser√° o acesso externo?',
                    'context': 'ALB para HTTP/HTTPS, NLB para TCP/UDP, sem load balancer para interno apenas',
                    'options': ['Application Load Balancer (HTTP)', 'Network Load Balancer (TCP)', 'Sem acesso externo']
                }
            ],
            's3': [
                {
                    'question': 'Precisa de website est√°tico ou apenas storage?',
                    'context': 'S3 pode hospedar sites com CloudFront para CDN global e melhor performance',
                    'options': ['Website est√°tico + CloudFront', 'Storage de arquivos apenas', 'Ambos']
                },
                {
                    'question': 'Qual classe de storage voc√™ precisa?',
                    'context': 'Standard para acesso frequente, IA para infrequente, Glacier para arquivo',
                    'options': ['Standard (acesso frequente)', 'Intelligent-Tiering (autom√°tico)', 'Infrequent Access (economia)']
                }
            ],
            'lambda': [
                {
                    'question': 'Qual runtime voc√™ vai usar?',
                    'context': 'Diferentes runtimes t√™m diferentes performance e cold start times',
                    'options': ['Python 3.11 (recomendado)', 'Node.js 18', 'Java 17']
                },
                {
                    'question': 'Qual ser√° o trigger principal?',
                    'context': 'Diferentes triggers t√™m diferentes configura√ß√µes e limites',
                    'options': ['API Gateway (REST API)', 'EventBridge (eventos)', 'S3 (upload de arquivos)']
                }
            ],
            'elasticache': [
                {
                    'question': 'Voc√™ prefere Redis ou Memcached?',
                    'context': 'Redis tem mais features (persist√™ncia, estruturas), Memcached √© mais simples',
                    'options': ['Redis (recomendado)', 'Memcached (simples)', 'Redis Cluster (escala)']
                }
            ]
        }
        
        return intelligent_fallbacks.get(service, [
            {
                'question': f'Qual configura√ß√£o voc√™ precisa para {service}?',
                'context': f'Diferentes configura√ß√µes de {service} t√™m diferentes trade-offs',
                'options': ['Configura√ß√£o b√°sica', 'Configura√ß√£o de produ√ß√£o', 'Configura√ß√£o customizada']
            }
        ])
    
    def _detect_primary_service(self, user_request: str) -> str:
        """Detecta o servi√ßo principal do requisito com melhor precis√£o"""
        request_lower = user_request.lower()
        
        # Mapeamento mais espec√≠fico
        service_keywords = {
            'rds': ['banco de dados', 'database', 'mysql', 'postgresql', 'rds', 'relacional', 'sql'],
            'dynamodb': ['nosql', 'dynamodb', 'chave-valor', 'key-value', 'document'],
            'ecs': ['container', 'docker', 'ecs', 'fargate', 'containerizar'],
            's3': ['storage', 'arquivo', 'bucket', 's3', 'website', 'static'],
            'lambda': ['serverless', 'fun√ß√£o', 'lambda', 'event', 'trigger'],
            'elasticache': ['cache', 'redis', 'memcached', 'elasticache', 'caching'],
            'vpc': ['rede', 'network', 'vpc', 'subnet', 'networking'],
            'ec2': ['instancia', 'instance', 'ec2', 'virtual machine', 'vm']
        }
        
        # Score por servi√ßo
        service_scores = {}
        for service, keywords in service_keywords.items():
            score = sum(1 for keyword in keywords if keyword in request_lower)
            if score > 0:
                service_scores[service] = score
        
        # Retornar servi√ßo com maior score
        if service_scores:
            return max(service_scores, key=service_scores.get)
        
        return 'general'
    
    async def _intelligent_fallback_analysis(self, user_request: str) -> Dict[str, Any]:
        """Fallback inteligente quando LLM falha completamente"""
        try:
            # An√°lise b√°sica de gaps
            questions = await self._generate_real_intelligent_questions(user_request, {'status': 'needs_clarification'})
            
            if questions:
                formatted_response = self._format_clarification_questions(questions, user_request)
                return {
                    'status': 'needs_clarification',
                    'response': formatted_response,
                    'questions': questions,
                    'confidence': 0.6,
                    'reasoning': 'Intelligent fallback analysis with MCP context',
                    'llm_used': False
                }
            
        except Exception as e:
            print(f"‚ö†Ô∏è Intelligent fallback error: {e}")
        
        # √öltimo fallback - sempre permite prosseguir
        return {
            'status': 'ready_to_generate',
            'confidence': 0.4,
            'reasoning': 'Emergency fallback - proceeding with available information'
        }
    
    def _format_clarification_questions(self, questions: List[Dict[str, Any]], user_request: str) -> str:
        """Formata perguntas de clarifica√ß√£o para o usu√°rio"""
        
        # Salvar perguntas na sess√£o conversacional
        try:
            from core.conversation_state_manager import ConversationStateManager
            conversation_manager = ConversationStateManager()
            session_id = conversation_manager.start_session()
            conversation_manager.add_clarification_questions(session_id, questions)
            print(f"üìù Added {len(questions)} questions to session {session_id}")
        except Exception as e:
            print(f"‚ö†Ô∏è Erro saving questions: {e}")
        
        # Formatar resposta com indicador de fonte
        llm_used = any(q.get('source') == 'llm_mcp' for q in questions)
        source_indicator = "üß† LLM+MCP" if llm_used else "ü§ñ Intelligent Fallback"
        
        response = f"ü§î **Preciso de mais detalhes sobre: '{user_request}'** ({source_indicator})\n\n"
        
        for i, q in enumerate(questions, 1):
            response += f"**{i}. {q['question']}**\n"
            if q.get('context'):
                response += f"üí° *{q['context']}*\n"
            if q.get('options'):
                for j, option in enumerate(q['options'], 1):
                    response += f"   {j}. {option}\n"
            response += "\n"
        
        response += "üìù **Responda com detalhes ou n√∫meros das op√ß√µes para prosseguir.**"
        
        return response
    
    def _is_answer_to_question(self, request: str) -> bool:
        """Check if request is an answer to a previous clarification question"""
        request_lower = request.lower()
        
        # Answer patterns expandidos
        answer_patterns = [
            'sim', 'n√£o', 'yes', 'no', 'rds', 'dynamodb', 'redis', 'memcached',
            'multi-az', 'single-az', 'p√∫blico', 'privado', 'vpc', 'internet',
            'fargate', 'ec2', 'lambda', 'mysql', 'postgresql', 'aurora',
            't3.', 't2.', 'm5.', 'c5.', 'r5.', # instance types
            'gb', 'tb', 'mb', # storage sizes
            '1', '2', '3', '4', '5', # option numbers
            'standard', 'infrequent', 'glacier', 'cloudfront'
        ]
        
        # Check if request contains answer-like patterns
        return any(pattern in request_lower for pattern in answer_patterns)
    
    def _has_sufficient_details(self, request: str) -> bool:
        """Check if request has sufficient details to bypass clarification"""
        request_lower = request.lower()
        
        # APENAS comandos MUITO espec√≠ficos com TODOS os detalhes necess√°rios
        ultra_specific_patterns = [
            # RDS com engine e configura√ß√£o espec√≠fica
            ('rds' in request_lower and ('mysql' in request_lower or 'postgresql' in request_lower) and 
             ('multi-az' in request_lower or 'single-az' in request_lower)),
            
            # DynamoDB com configura√ß√£o espec√≠fica
            ('dynamodb' in request_lower and ('on-demand' in request_lower or 'provisioned' in request_lower)),
            
            # ECS com Fargate/EC2 espec√≠fico
            ('ecs' in request_lower and ('fargate' in request_lower or 'ec2' in request_lower) and
             'alb' in request_lower),
            
            # Lambda com runtime espec√≠fico
            ('lambda' in request_lower and ('python' in request_lower or 'node' in request_lower) and 
             ('api gateway' in request_lower or 'eventbridge' in request_lower)),
        ]
        
        return any(pattern for pattern in ultra_specific_patterns)
